import sys
import json
import shutil
import cv2
import numpy as np
import pandas as pd
import gc
import torch
from pathlib import Path
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader

try:
    from mmpose.apis import init_model
    from mmpose.utils import register_all_modules
    from mmpose.structures import PoseDataSample, merge_data_samples
    from mmengine.dataset import Compose
except ImportError:
    print("âŒ MMPose ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    sys.exit(1)

try:
    from functions.extract_bbox_and_id import extract_bbox_and_id
except ImportError:
    sys.path.append(str(Path(__file__).resolve().parent.parent))
    from functions.extract_bbox_and_id import extract_bbox_and_id

def to_py(obj):
    if isinstance(obj, np.ndarray): return obj.tolist()
    if isinstance(obj, (np.floating,)): return float(obj)
    if isinstance(obj, (np.integer,)): return int(obj)
    if isinstance(obj, dict): return {k: to_py(v) for k, v in obj.items()}
    if isinstance(obj, (list, tuple)): return [to_py(v) for v in obj]
    return obj

# ============================================================
# 0ï¸âƒ£ [NEW] BBox ì•ˆì •í™” ë§¤ë‹ˆì € (í•µì‹¬ ë¡œì§)
# ============================================================
class DynamicBBoxManager:
    def __init__(self, expand_ratio=1.0, min_occupancy=0.7, smoothing=0.2, cooldown=30):
        """
        Args:
            expand_ratio (float): í™•ì¥ ë¹„ìœ¨ (ê¸°ë³¸ 1.2ë°°)
            min_occupancy (float): ìµœì†Œ ì ìœ ìœ¨ (0.7 = 70%). ì´ë³´ë‹¤ ì‘ì•„ì§€ë©´(ê³µê°„ë‚­ë¹„) ì¶•ì†Œ.
            smoothing (float): ë³€ê²½ ì‹œ ë¶€ë“œëŸ¬ìš´ ì´ë™ ê³„ìˆ˜ (EMA)
            cooldown (int): ì¦ì€ í¬ê¸° ë³€ê²½ ë°©ì§€ìš© íƒ€ì´ë¨¸
        """
        self.expand_ratio = expand_ratio
        # ì ìœ ìœ¨ 70% ì´í•˜ == ì—¬ë°± 30% ì´ìƒ (1.0 - 0.7 = 0.3)
        self.margin_threshold = 1.0 - min_occupancy 
        self.alpha = smoothing
        
        # ìƒíƒœ ë³€ìˆ˜
        self.current_box = None 
        self.cooldown_max = cooldown
        self.cooldown_timer = 0

    def update(self, mask_bbox):
        # 1. ì´ˆê¸°í™” (ì²« í”„ë ˆì„)
        if self.current_box is None:
            self.current_box = self._make_target_box(mask_bbox, self.expand_ratio)
            return self.current_box

        # ì¿¨íƒ€ì„ ê°ì†Œ
        if self.cooldown_timer > 0:
            self.cooldown_timer -= 1

        # 2. ìƒíƒœ íŒë‹¨ (ìš°ì„ ìˆœìœ„: í™•ì¥ > ì¶•ì†Œ > ìœ ì§€)
        
        # [Priority 1] í™•ì¥: ë§ˆìŠ¤í¬ê°€ ë°•ìŠ¤ë¥¼ ëš«ê³  ë‚˜ê° (ê°ì²´ ì˜ë¦¼ ë°©ì§€) -> ì¦‰ì‹œ ì‹¤í–‰
        if self._is_out_of_bound(mask_bbox, self.current_box):
            target_box = self._make_target_box(mask_bbox, self.expand_ratio)
            self.current_box = self._smooth_update(self.current_box, target_box)
            self.cooldown_timer = self.cooldown_max # ë³€ê²½ í›„ íœ´ì‹
            
        # [Priority 2] ì¶•ì†Œ: ì ìœ ìœ¨ì´ 70% ì´í•˜ë¡œ ë–¨ì–´ì§ (ê³µê°„ ë‚­ë¹„) -> ì¿¨íƒ€ì„ ì¢…ë£Œ í›„ ì‹¤í–‰
        elif self.cooldown_timer == 0 and \
             self._has_excessive_margin(mask_bbox, self.current_box, self.margin_threshold):
            
            target_box = self._make_target_box(mask_bbox, self.expand_ratio)
            self.current_box = self._smooth_update(self.current_box, target_box)
            self.cooldown_timer = self.cooldown_max
            
        # [Priority 3] ìœ ì§€ (Dead Zone): Jitter ë°©ì§€ êµ¬ê°„
        else:
            pass # ì•„ë¬´ê²ƒë„ ì•ˆ í•¨ (í˜„ì¬ ë°•ìŠ¤ ê³ ì •)

        return self.current_box

    # --- ë‚´ë¶€ ë¡œì§ ---
    def _make_target_box(self, box, ratio):
        """ì¤‘ì‹¬ ê¸°ì¤€ í™•ì¥"""
        x1, y1, x2, y2 = box
        w, h = x2 - x1, y2 - y1
        cx, cy = x1 + w/2, y1 + h/2
        nw, nh = w * ratio, h * ratio
        return [cx - nw/2, cy - nh/2, cx + nw/2, cy + nh/2]

    def _is_out_of_bound(self, mask, view):
        """ë§ˆìŠ¤í¬ê°€ ë·° ë°•ìŠ¤ë¥¼ ë²—ì–´ë‚¬ëŠ”ê°€?"""
        pad = 0.1 # ë¶€ë™ì†Œìˆ˜ì  ì˜¤ì°¨ ë°©ì§€
        return (mask[0] < view[0]-pad) or (mask[1] < view[1]-pad) or \
               (mask[2] > view[2]+pad) or (mask[3] > view[3]+pad)

    def _has_excessive_margin(self, mask, view, threshold):
        """ë¹ˆ ê³µê°„ ë¹„ìœ¨ì´ ì„ê³„ì¹˜(30%)ë¥¼ ë„˜ëŠ”ê°€?"""
        view_w = view[2] - view[0]
        view_h = view[3] - view[1]
        mask_w = mask[2] - mask[0]
        mask_h = mask[3] - mask[1]
        
        if view_w <= 0 or view_h <= 0: return True
        
        # ê°€ë¡œ ì—¬ë°± or ì„¸ë¡œ ì—¬ë°± ì¤‘ í•˜ë‚˜ë¼ë„ í¬ë©´ True
        margin_w = 1.0 - (mask_w / view_w)
        margin_h = 1.0 - (mask_h / view_h)
        return (margin_w > threshold) or (margin_h > threshold)

    def _smooth_update(self, current, target):
        """EMA ìŠ¤ë¬´ë”©"""
        return [c * (1 - self.alpha) + t * self.alpha for c, t in zip(current, target)]

def stabilize_bboxes(tasks):
    """ì „ì²´ íƒœìŠ¤í¬ë¥¼ ìˆœíšŒí•˜ë©° BBoxë¥¼ ì•ˆì •í™”ëœ ê°’ìœ¼ë¡œ ë®ì–´ì”Œì›€"""
    print("ğŸŒŠ BBox ì•ˆì •í™” ë¡œì§ ì ìš© ì¤‘ (Hysteresis)...")
    managers = {} # {obj_id: Manager}

    # tasksëŠ” ì‹œê°„ìˆœ ì •ë ¬ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
    for sam_file, file_name, objects in tasks:
        for obj in objects:
            if not obj.get('bbox'): continue
            
            oid = obj['id']
            raw_bbox = obj['bbox']
            
            # IDë³„ ë§¤ë‹ˆì € ìƒì„± (1.2ë°° í™•ì¥, 70% ì ìœ ìœ¨ ìœ ì§€, ìŠ¤ë¬´ë”© 0.2)
            if oid not in managers:
                managers[oid] = DynamicBBoxManager(expand_ratio=1.1, min_occupancy=0.7, smoothing=0.2)
            
            # ì•ˆì •í™”ëœ ë°•ìŠ¤ ê³„ì‚°
            stable_bbox = managers[oid].update(raw_bbox)
            
            # ğŸŒŸ ì›ë³¸ ë°ì´í„° êµì²´!
            obj['bbox'] = stable_bbox
            
    return tasks

# ============================================================
# 1ï¸âƒ£ Dataset: Gray Padding (Letterbox) ì ìš© + Batch ì¤€ë¹„
# ============================================================
class SapiensBatchDataset(Dataset):
    def __init__(self, tasks, frame_dir, input_size=(1024, 768)):
        self.frame_dir = Path(frame_dir)
        self.items = []
        self.input_size = input_size # (W, H)
        
        # ì´ë¯¸ stabilize_bboxesë¥¼ ê±°ì³ì„œ bboxê°€ ë³´ì •ëœ tasksë¥¼ ë°›ìŒ
        for sam_file, file_name, objects in tasks:
            f_idx = int(sam_file.stem) if sam_file.stem.isdigit() else 0
            for obj in objects:
                if obj.get('bbox'):
                    self.items.append({
                        'stem': sam_file.stem,
                        'file_name': file_name,
                        'frame_idx': f_idx, 
                        'obj_id': obj['id'], 
                        'bbox': obj['bbox'] # [x1, y1, x2, y2] (ì´ë¯¸ ì•ˆì •í™”ë¨)
                    })

    def __len__(self): return len(self.items)

    def __getitem__(self, idx):
        item = self.items[idx]
        img = cv2.imread(str(self.frame_dir / item['file_name']))
        if img is None: return None
        
        # --- 1. Crop (ì´ë¯¸ ì•ˆì •í™”ëœ BBox ì‚¬ìš©) ---
        # ì£¼ì˜: DynamicBBoxManagerì—ì„œ ì´ë¯¸ 1.2ë°° í™•ì¥ì„ í¬í•¨í•˜ê³  ìˆìœ¼ë¯€ë¡œ,
        # ì—¬ê¸°ì„œ ì¶”ê°€ë¡œ 1.2ë°°ë¥¼ ë˜ ê³±í•˜ë©´ ì•ˆ ë¨! (ì¤‘ë³µ í™•ì¥ ë°©ì§€)
        x1, y1, x2, y2 = item['bbox']
        img_h, img_w = img.shape[:2]
        
        # BBoxëŠ” floatì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ int ë³€í™˜
        bw, bh = x2 - x1, y2 - y1
        cx, cy = x1 + bw / 2, y1 + bh / 2
        
        # ì´ë¯¸ì§€ ê²½ê³„ ë„˜ì§€ ì•Šë„ë¡ Clipping
        nx1, ny1 = max(0, int(x1)), max(0, int(y1))
        nx2, ny2 = min(img_w, int(x2)), min(img_h, int(y2))
        
        crop = img[ny1:ny2, nx1:nx2].copy()
        
        # ë§Œì•½ cropì´ ë¹„ì–´ìˆìœ¼ë©´(ë°•ìŠ¤ê°€ ì´ë¯¸ì§€ ë°–) ì˜ˆì™¸ì²˜ë¦¬
        if crop.size == 0: return None, None 

        # --- 2. Gray Padding (Letterbox Resize) ---
        target_w, target_h = self.input_size
        h, w = crop.shape[:2]
        
        # ë¹„ìœ¨ ìœ ì§€ ìŠ¤ì¼€ì¼ ê³„ì‚°
        scale = min(target_w / w, target_h / h)
        new_w, new_h = int(w * scale), int(h * scale)
        
        resized = cv2.resize(crop, (new_w, new_h))
        
        # íšŒìƒ‰(128) ìº”ë²„ìŠ¤ ìƒì„±
        canvas = np.full((target_h, target_w, 3), 128, dtype=np.uint8)
        
        # ì¤‘ì•™ ì •ë ¬
        pad_x = (target_w - new_w) // 2
        pad_y = (target_h - new_h) // 2
        
        canvas[pad_y:pad_y+new_h, pad_x:pad_x+new_w] = resized
        
        # --- 3. Normalize & ToTensor ---
        input_img = canvas.astype(np.float32)
        mean = np.array([123.675, 116.28, 103.53], dtype=np.float32)
        std = np.array([58.395, 57.12, 57.375], dtype=np.float32)
        input_img = (input_img - mean) / std
        input_img = input_img.transpose(2, 0, 1)
        
        # Meta ì •ë³´
        meta = {
            'stem': item['stem'],
            'file_name': item['file_name'],
            'frame_idx': item['frame_idx'],
            'obj_id': item['obj_id'],
            'crop_bbox': [nx1, ny1, nx2, ny2],
            'padding': [pad_x, pad_y],
            'scale_factor': scale,
            'input_size': self.input_size
        }
        
        return torch.from_numpy(input_img), meta

def collate_fn(batch):
    batch = [b for b in batch if b is not None and b[0] is not None] # None í•„í„°ë§ ê°•í™”
    if not batch: return None
    return torch.stack([b[0] for b in batch]), [b[1] for b in batch]

# ============================================================
# 2ï¸âƒ£ Inference: Full Model API ì‚¬ìš© + Batch ì²˜ë¦¬
# ============================================================
def run_sapiens_batch_inference(frame_dir, sam_dir, output_dir, config_path, ckpt_path, batch_size=8):
    frame_dir, sam_dir, output_dir = Path(frame_dir), Path(sam_dir), Path(output_dir)
    if output_dir.exists(): shutil.rmtree(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    register_all_modules()
    
    # 1. SAM JSON ìŠ¤ìº”
    sam_files = sorted(list(sam_dir.glob("*.json")))
    tasks = []
    print("ğŸ“‚ SAM JSON ìŠ¤ìº” ì¤‘...")
    for sam_file in tqdm(sam_files):
        file_name, objects = extract_bbox_and_id(str(sam_file))
        if (frame_dir / file_name).exists():
            tasks.append((sam_file, file_name, objects))

    # ğŸŒŸ [NEW] BBox ì•ˆì •í™” ì ìš© (Inference ì „ì²˜ë¦¬)
    tasks = stabilize_bboxes(tasks)

    # 2. ëª¨ë¸ ë¡œë“œ
    print("ğŸš€ Sapiens ëª¨ë¸ ë¡œë“œ (Batch Mode)...")
    model = init_model(str(config_path), str(ckpt_path), device='cuda:0')
    model.eval()

    # 3. DataLoader ì¤€ë¹„
    dataset = SapiensBatchDataset(tasks, frame_dir, input_size=(1024, 768))
    loader = DataLoader(dataset, batch_size=batch_size, num_workers=8, shuffle=False, collate_fn=collate_fn, pin_memory=True)
    
    print(f"âš¡ Batch Inference ì‹œì‘ (Total Objects: {len(dataset)})")
    
    # 4. Inference Loop
    for batch in tqdm(loader, desc="Processing"):
        if batch is None: continue
        inputs, metas = batch
        inputs = inputs.to('cuda', non_blocking=True)

        with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16):
            feats = model.extract_feat(inputs)
            
            batch_data_samples = [
                PoseDataSample(metainfo=dict(input_size=m['input_size'])) 
                for m in metas
            ]
            preds = model.head.predict(feats, batch_data_samples)

        # 5. ì¢Œí‘œ ë³µì› ë° ì €ì¥
        for i, pred_sample in enumerate(preds):
            meta = metas[i]
            
            if hasattr(pred_sample, 'pred_instances'):
                instances = pred_sample.pred_instances
            else:
                instances = pred_sample
            
            kpts_crop = instances.keypoints
            scores = instances.keypoint_scores
            
            if kpts_crop.ndim == 3:
                kpts_crop = kpts_crop[0]
                scores = scores[0]
            
            pad_x, pad_y = meta['padding']
            scale = meta['scale_factor']
            off_x, off_y = meta['crop_bbox'][:2]
            
            final_kpts = []
            if isinstance(kpts_crop, torch.Tensor): kpts_crop = kpts_crop.cpu().numpy()
            if isinstance(scores, torch.Tensor): scores = scores.cpu().numpy()

            for (cx, cy), score in zip(kpts_crop, scores):
                x_nopad = cx - pad_x
                y_nopad = cy - pad_y
                fx = (x_nopad / scale) + off_x
                fy = (y_nopad / scale) + off_y
                final_kpts.append([float(fx), float(fy), float(score)])
            
            crop_bbox = [float(v) for v in meta['crop_bbox']]
            instance_item = {
                "instance_id": int(meta['obj_id']),
                "keypoints": final_kpts,
                "keypoint_scores": scores.tolist(),
                "bbox": crop_bbox
            }
            
            save_path = output_dir / f"{meta['stem']}.json"
            
            if save_path.exists():
                with open(save_path, "r", encoding="utf-8") as f:
                    data_j = json.load(f)
                data_j['instance_info'].append(instance_item)
            else:
                data_j = {
                    "frame_index": meta['frame_idx'],
                    "file_name": meta['file_name'],
                    "meta_info": to_py(model.dataset_meta),
                    "instance_info": [instance_item]
                }
            
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(data_j, f, ensure_ascii=False, indent=2)
        
        del inputs, feats, preds, batch_data_samples # ë³€ìˆ˜ ì‚­ì œ
        gc.collect()            # íŒŒì´ì¬ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        torch.cuda.empty_cache() # GPU ìºì‹œ ë¹„ìš°ê¸° (ì†ë„ëŠ” ì•½ê°„ ëŠë ¤ì§€ì§€ë§Œ ì•ˆì „í•¨)
                
    return len(list(output_dir.glob("*.json")))

# ============================================================
# Main ì‹¤í–‰ë¶€
# ============================================================
import sys
sys.path.append(str(Path(__file__).resolve().parent.parent))
from functions.generate_skeleton_video import generate_skeleton_video

if __name__ == "__main__":
    DATA_DIR = Path("/workspace/nas203/ds_RehabilitationMedicineData/IDs/tojihoo/data")
    
    df = pd.read_csv(DATA_DIR / "metadata.csv")
    target_df = df[df['n_json'] == 0]

    # ğŸ” enumerateë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆœì„œ(i)ì™€ ë°ì´í„°(index, row)ë¥¼ ë¶„ë¦¬
    for i, (index, row) in enumerate(target_df.iterrows(), start=1):
        
        print(f"\n================= ì²˜ë¦¬ ì‹œì‘: {row['common_path']} =================")
        # index+1 ëŒ€ì‹  i (í˜„ì¬ ë°˜ë³µ íšŸìˆ˜)ë¥¼ ì‚¬ìš©
        print(f"ì§„í–‰ìƒí™©: {i}/{len(target_df)} (Original Index: {index})") 
        
        COMMON_PATH = row['common_path']
        COMMON_PATH = row['common_path']

        FRAME_DIR = DATA_DIR / "1_FRAME" / COMMON_PATH
        SAM_DIR = DATA_DIR / "8_SAM" / COMMON_PATH
        OUTPUT_DIR = DATA_DIR / "2_KEYPOINTS" / COMMON_PATH

        # COCO 133ì  ê¸°ë°˜    
        # CONFIG = "/workspace/nas203/ds_RehabilitationMedicineData/IDs/tojihoo/ASAN_01_mini_sapiens_labeling/configs/sapiens/sapiens_0.3b-210e_coco_wholebody-1024x768.py"
        # CKPT = DATA_DIR / "checkpoints/sapiens/pose/sapiens_0.3b_coco_wholebody_best_coco_wholebody_AP_620.pth"

        # COCO 17ì  ê¸°ë°˜ 0.3b
        CONFIG = "/workspace/nas203/ds_RehabilitationMedicineData/IDs/tojihoo/ASAN_01_mini_sapiens_labeling/configs/sapiens/sapiens_0.3b-210e_coco-1024x768.py"
        CKPT = DATA_DIR / "checkpoints/sapiens/pose/sapiens_0.3b_coco_best_coco_AP_796.pth"

        # COCO 17ì  ê¸°ë°˜ 0.6b
        # CONFIG = "/workspace/nas203/ds_RehabilitationMedicineData/IDs/tojihoo/ASAN_01_mini_sapiens_labeling/configs/sapiens/sapiens_0.6b-210e_coco-1024x768.py"
        # CKPT = DATA_DIR / "checkpoints/sapiens/pose/sapiens_0.6b_coco_best_coco_AP_812.pth"

        print(f"\noutput_dir: {OUTPUT_DIR}\n")
        # batch_sizeë¥¼ 32~64 ì •ë„ë¡œ ë†’ì—¬ë³´ì„¸ìš” (VRAM í—ˆìš© ì‹œ)
        count = run_sapiens_batch_inference(FRAME_DIR, SAM_DIR, OUTPUT_DIR, CONFIG, CKPT, batch_size=50)
        print(f"âœ… ì™„ë£Œ: {count}ê°œ JSON ìƒì„±")

        df.at[index, 'n_json'] = count
        df.to_csv(DATA_DIR / "metadata.csv", index=False)
        print(f"ğŸ“Š metadata.csv ì—…ë°ì´íŠ¸ ì™„ë£Œ")

        MP4_PATH = DATA_DIR / f"3_MP4/{COMMON_PATH}.mp4"

        # ê²°ê³¼ ê²€ì¦ ì˜ìƒ ìƒì„±
        generate_skeleton_video(
            frame_dir=FRAME_DIR,
            kpt_dir=OUTPUT_DIR,
            output_path=str(MP4_PATH),
            conf_threshold=0
        )