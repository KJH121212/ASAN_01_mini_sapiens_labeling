import sys
import json
import shutil
import cv2
import numpy as np
import pandas as pd
import torch
from pathlib import Path
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader

# --- MMPose ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
try:
    from mmpose.apis import init_model
    from mmpose.utils import register_all_modules
    from mmpose.structures import PoseDataSample, merge_data_samples
    from mmengine.dataset import Compose
except ImportError:
    print("âŒ MMPose ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    sys.exit(1)

try:
    from functions.extract_bbox_and_id import extract_bbox_and_id
except ImportError:
    sys.path.append(str(Path(__file__).resolve().parent.parent))
    from functions.extract_bbox_and_id import extract_bbox_and_id

def to_py(obj):
    if isinstance(obj, np.ndarray): return obj.tolist()
    if isinstance(obj, (np.floating,)): return float(obj)
    if isinstance(obj, (np.integer,)): return int(obj)
    if isinstance(obj, dict): return {k: to_py(v) for k, v in obj.items()}
    if isinstance(obj, (list, tuple)): return [to_py(v) for v in obj]
    return obj

# ============================================================
# 1ï¸âƒ£ Dataset: Gray Padding (Letterbox) ì ìš© + Batch ì¤€ë¹„
# ============================================================
class SapiensBatchDataset(Dataset):
    def __init__(self, tasks, frame_dir, input_size=(1024, 768)):
        self.frame_dir = Path(frame_dir)
        self.items = []
        self.input_size = input_size # (W, H)
        
        # SAM JSONì—ì„œ ìœ íš¨í•œ BBoxê°€ ìˆëŠ” ê°ì²´ë§Œ ë¦¬ìŠ¤íŠ¸ì—…
        for sam_file, file_name, objects in tasks:
            f_idx = int(sam_file.stem) if sam_file.stem.isdigit() else 0
            for obj in objects:
                if obj.get('bbox'):
                    self.items.append({
                        'stem': sam_file.stem,
                        'file_name': file_name,
                        'frame_idx': f_idx, 
                        'obj_id': obj['id'], 
                        'bbox': obj['bbox'] # [x1, y1, x2, y2]
                    })

    def __len__(self): return len(self.items)

    def __getitem__(self, idx):
        item = self.items[idx]
        img = cv2.imread(str(self.frame_dir / item['file_name']))
        if img is None: return None
        
        # --- 1. Crop (SAM BBox + 1.2ë°° í™•ì¥) ---
        x1, y1, x2, y2 = item['bbox']
        img_h, img_w = img.shape[:2]
        
        bw, bh = x2 - x1, y2 - y1
        cx, cy = x1 + bw / 2, y1 + bh / 2
        nw, nh = bw * 1.2, bh * 1.2 # 1.2ë°° í™•ì¥
        
        # ì´ë¯¸ì§€ ê²½ê³„ ë„˜ì§€ ì•Šë„ë¡ Clipping
        nx1, ny1 = max(0, int(cx - nw / 2)), max(0, int(cy - nh / 2))
        nx2, ny2 = min(img_w, int(cx + nw / 2)), min(img_h, int(cy + nh / 2))
        
        crop = img[ny1:ny2, nx1:nx2].copy()
        
        # --- 2. Gray Padding (Letterbox Resize) ---
        # 
        target_w, target_h = self.input_size
        h, w = crop.shape[:2]
        
        # ë¹„ìœ¨ ìœ ì§€ ìŠ¤ì¼€ì¼ ê³„ì‚°
        scale = min(target_w / w, target_h / h)
        new_w, new_h = int(w * scale), int(h * scale)
        
        resized = cv2.resize(crop, (new_w, new_h))
        
        # íšŒìƒ‰(128) ìº”ë²„ìŠ¤ ìƒì„±
        canvas = np.full((target_h, target_w, 3), 128, dtype=np.uint8)
        
        # ì¤‘ì•™ ì •ë ¬
        pad_x = (target_w - new_w) // 2
        pad_y = (target_h - new_h) // 2
        
        canvas[pad_y:pad_y+new_h, pad_x:pad_x+new_w] = resized
        
        # --- 3. Normalize & ToTensor ---
        # MMPose í‘œì¤€ Mean/Std ì ìš©
        input_img = canvas.astype(np.float32) # ë°ì´í„°ë¥¼ ì†Œìˆ˜ì  ì—°ì‚°ì´ ê°€ëŠ¥í•œ float32 íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        mean = np.array([123.675, 116.28, 103.53], dtype=np.float32) # ImageNet ë°ì´í„°ì…‹ì˜ RGB ì±„ë„ë³„ í‰ê· ê°’ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        std = np.array([58.395, 57.12, 57.375], dtype=np.float32) # ImageNet ë°ì´í„°ì…‹ì˜ RGB ì±„ë„ë³„ í‘œì¤€í¸ì°¨ ê°’ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        input_img = (input_img - mean) / std # (ì…ë ¥ê°’ - í‰ê· ) / í‘œì¤€í¸ì°¨ ê³µì‹ì„ í†µí•´ ë°ì´í„°ë¥¼ ì •ê·œí™”(Normalization)í•©ë‹ˆë‹¤.
        input_img = input_img.transpose(2, 0, 1) # ì´ë¯¸ì§€ ë°°ì—´ ìˆœì„œë¥¼ [ë†’ì´, ë„ˆë¹„, ì±„ë„]ì—ì„œ [ì±„ë„, ë†’ì´, ë„ˆë¹„]ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
        
        # Meta ì •ë³´ (ë‚˜ì¤‘ì— ì¢Œí‘œ ë³µì›ìš©)
        meta = {
            'stem': item['stem'],
            'file_name': item['file_name'],
            'frame_idx': item['frame_idx'],
            'obj_id': item['obj_id'],
            'crop_bbox': [nx1, ny1, nx2, ny2], # ì›ë³¸ ì´ë¯¸ì§€ì—ì„œì˜ Crop ìœ„ì¹˜
            'padding': [pad_x, pad_y],         # ì¶”ê°€ëœ íŒ¨ë”© ì–‘
            'scale_factor': scale,             # ë¦¬ì‚¬ì´ì¦ˆ ë¹„ìœ¨
            'input_size': self.input_size      # ëª¨ë¸ ì…ë ¥ í¬ê¸°
        }
        
        return torch.from_numpy(input_img), meta

def collate_fn(batch):
    batch = [b for b in batch if b is not None]
    if not batch: return None
    return torch.stack([b[0] for b in batch]), [b[1] for b in batch]

# ============================================================
# 2ï¸âƒ£ Inference: Full Model API ì‚¬ìš© + Batch ì²˜ë¦¬
# ============================================================
def run_sapiens_batch_inference(frame_dir, sam_dir, output_dir, config_path, ckpt_path, batch_size=8):
    frame_dir, sam_dir, output_dir = Path(frame_dir), Path(sam_dir), Path(output_dir)
    if output_dir.exists(): shutil.rmtree(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    register_all_modules()
    
    # 1. SAM JSON ìŠ¤ìº”
    sam_files = sorted(list(sam_dir.glob("*.json")))
    tasks = []
    print("ğŸ“‚ SAM JSON ìŠ¤ìº” ì¤‘...")
    for sam_file in tqdm(sam_files):
        file_name, objects = extract_bbox_and_id(str(sam_file))
        if (frame_dir / file_name).exists():
            tasks.append((sam_file, file_name, objects))

    # 2. ëª¨ë¸ ë¡œë“œ
    print("ğŸš€ Sapiens ëª¨ë¸ ë¡œë“œ (Batch Mode)...")
    model = init_model(str(config_path), str(ckpt_path), device='cuda:0')
    model.eval()

    # 3. DataLoader ì¤€ë¹„
    dataset = SapiensBatchDataset(tasks, frame_dir, input_size=(1024, 768))
    loader = DataLoader(dataset, batch_size=batch_size, num_workers=8, shuffle=False, collate_fn=collate_fn, pin_memory=True)
    
    print(f"âš¡ Batch Inference ì‹œì‘ (Total Objects: {len(dataset)})")
    
    # 4. Inference Loop
    for batch in tqdm(loader, desc="Processing"):
        if batch is None: continue
        inputs, metas = batch
        inputs = inputs.to('cuda', non_blocking=True) # (B, 3, 768, 1024)

        # AMP ì ìš© (ë©”ëª¨ë¦¬ ì ˆì•½) Gradient ì €ì¥ í•˜ì§€ ë§ê³  BF16 ê³„ì‚°ë²• ì‚¬ìš©
        with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16):
            feats = model.extract_feat(inputs)
            
            # Head ì˜ˆì¸¡
            batch_data_samples = [
                PoseDataSample(metainfo=dict(input_size=m['input_size'])) 
                for m in metas
            ]
            preds = model.head.predict(feats, batch_data_samples)

        # 5. ì¢Œí‘œ ë³µì› ë° ì €ì¥
        for i, pred_sample in enumerate(preds):
            meta = metas[i]
            
            # ğŸŒŸ [ìˆ˜ì •] InstanceData vs PoseDataSample í˜¸í™˜ ì²˜ë¦¬
            if hasattr(pred_sample, 'pred_instances'):
                # PoseDataSample ê°ì²´ì¸ ê²½ìš°
                instances = pred_sample.pred_instances
            else:
                # InstanceData ê°ì²´ ìì²´ì¸ ê²½ìš° (í˜„ì¬ ì—ëŸ¬ ìƒí™©)
                instances = pred_sample
            
            # Keypoints ì¶”ì¶œ
            kpts_crop = instances.keypoints
            scores = instances.keypoint_scores
            
            # ì°¨ì› í™•ì¸: (1, K, 2) í˜•íƒœë¼ë©´ ë°°ì¹˜ ì°¨ì› ì œê±°
            if kpts_crop.ndim == 3:
                kpts_crop = kpts_crop[0]
                scores = scores[0]
            
            # --- ì´í•˜ ì¢Œí‘œ ë³µì› ë¡œì§ ë™ì¼ ---
            pad_x, pad_y = meta['padding']
            scale = meta['scale_factor']
            off_x, off_y = meta['crop_bbox'][:2]
            
            final_kpts = []
            # Tensorì¼ ê²½ìš° CPUë¡œ ì´ë™
            if isinstance(kpts_crop, torch.Tensor): kpts_crop = kpts_crop.cpu().numpy()
            if isinstance(scores, torch.Tensor): scores = scores.cpu().numpy()

            for (cx, cy), score in zip(kpts_crop, scores):
                # 1. íŒ¨ë”© ì œê±°
                x_nopad = cx - pad_x
                y_nopad = cy - pad_y
                # 2. ìŠ¤ì¼€ì¼ ë³µì›
                fx = (x_nopad / scale) + off_x
                fy = (y_nopad / scale) + off_y
                final_kpts.append([float(fx), float(fy), float(score)])
            
            crop_bbox = [float(v) for v in meta['crop_bbox']]
            instance_item = {
                "instance_id": int(meta['obj_id']),
                "keypoints": final_kpts,
                "keypoint_scores": scores.tolist(),
                "bbox": crop_bbox
            }
            
            save_path = output_dir / f"{meta['stem']}.json"
            
            if save_path.exists():
                with open(save_path, "r", encoding="utf-8") as f:
                    data_j = json.load(f)
                data_j['instance_info'].append(instance_item)
            else:
                data_j = {
                    "frame_index": meta['frame_idx'],
                    "file_name": meta['file_name'],
                    "meta_info": to_py(model.dataset_meta),
                    "instance_info": [instance_item]
                }
            
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(data_j, f, ensure_ascii=False, indent=2)
                
    return len(list(output_dir.glob("*.json")))

# ============================================================
# Main ì‹¤í–‰ë¶€
# ============================================================
import sys
sys.path.append(str(Path(__file__).resolve().parent.parent))
from functions.generate_skeleton_video import generate_skeleton_video

if __name__ == "__main__":
    DATA_DIR = Path("/workspace/nas203/ds_RehabilitationMedicineData/IDs/tojihoo/data")
    
    df = pd.read_csv(DATA_DIR / "metadata.csv")
    for target in range(705, 710):
        COMMON_PATH = df['common_path'][target]

        FRAME_DIR = DATA_DIR / "1_FRAME" / COMMON_PATH
        SAM_DIR = DATA_DIR / "8_SAM" / COMMON_PATH
        # v5.0: Full Model + Batch + SAM BBox + Gray Padding
        OUTPUT_DIR = DATA_DIR / "test" / COMMON_PATH / "v5.0_17kpt_full"

        # COCO 133ì  ê¸°ë°˜    
        # CONFIG = "/workspace/nas203/ds_RehabilitationMedicineData/IDs/tojihoo/ASAN_01_mini_sapiens_labeling/configs/sapiens/sapiens_0.3b-210e_coco_wholebody-1024x768.py"
        # CKPT = DATA_DIR / "checkpoints/sapiens/pose/sapiens_0.3b_coco_wholebody_best_coco_wholebody_AP_620.pth"

        # COCO 17ì  ê¸°ë°˜
        CONFIG = "/workspace/nas203/ds_RehabilitationMedicineData/IDs/tojihoo/ASAN_01_mini_sapiens_labeling/configs/sapiens/sapiens_0.3b-210e_coco-1024x768.py"
        CKPT = DATA_DIR / "checkpoints/sapiens/pose/sapiens_0.3b_coco_best_coco_AP_796.pth"

        print(f"\noutput_dir: {OUTPUT_DIR}\n")
        # batch_sizeë¥¼ 32~64 ì •ë„ë¡œ ë†’ì—¬ë³´ì„¸ìš” (VRAM í—ˆìš© ì‹œ)
        # count = run_sapiens_batch_inference(FRAME_DIR, SAM_DIR, OUTPUT_DIR, CONFIG, CKPT, batch_size=20)
        # print(f"âœ… ì™„ë£Œ: {count}ê°œ JSON ìƒì„±")

        # ê²°ê³¼ ê²€ì¦ ì˜ìƒ ìƒì„±
        generate_skeleton_video(
            frame_dir=FRAME_DIR,
            kpt_dir=OUTPUT_DIR,
            output_path=str(f"{OUTPUT_DIR}_007_conf.mp4"),
            conf_threshold=0
        )